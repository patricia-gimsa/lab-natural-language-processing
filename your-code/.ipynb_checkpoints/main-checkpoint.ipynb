{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[32m      2\u001b[39m display(HTML(\u001b[33m\"\u001b[39m\u001b[33m<style>.container \u001b[39m\u001b[33m{\u001b[39m\u001b[33m width:100\u001b[39m\u001b[33m%\u001b[39m\u001b[33m !important; }</style>\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\display.py)"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 5 rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1000 non-null   object\n",
      " 1   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show column names and check for missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    558\n",
       "1    442\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many spam and ham messages are in the dataset\n",
    "data['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide the data into training and test sets\n",
    "# 80% for training, 20% for testing\n",
    "# We use the 'text' column as features (X) and 'label' column as target (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'],   # input features \n",
    "    data['label'],   # target labels (0 or 1)\n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=0     # reproducible split\n",
    ")\n",
    "\n",
    "# Show the number of samples in each set\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: nltk in c:\\users\\usuario\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "# Import the 'string' module to access punctuation characters like . , ? ! etc.\n",
    "import string\n",
    "\n",
    "# Import the list of common English stopwords (like 'the', 'and', 'is') from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Print all punctuation characters to see what will be removed\n",
    "print(string.punctuation)\n",
    "\n",
    "# Print a small sample of English stopwords (just to inspect them)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "\n",
    "# Import the SnowballStemmer to reduce words to their root form (e.g. 'running' → 'run')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Create a stemmer object for the English language\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we imported useful NLP tools from the nltk and string libraries.\n",
    "# We listed punctuation symbols and a sample of English stopwords to understand what will be removed later.\n",
    "# We also initialized a stemmer, which will help reduce words to their root form during preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to clean HTML content from the text\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript and CSS blocks (e.g. <script> ... </script>, <style> ... </style>)\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Remove HTML comments (e.g. <!-- comment -->)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove remaining HTML tags (e.g. <b>, <a href=\"...\">, </div>)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Return the cleaned plain text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned message from training set:\n",
      "H Thursday February 4 2010 7:09 PM'sbwhoeopRe: URGENT GOOD NEWs ON NI!!!! SidB6That is wonderful! Kudos to Gordon and Shaun.\n"
     ]
    }
   ],
   "source": [
    "# Apply the clean_html function to both training and test sets\n",
    "X_train = X_train.apply(clean_html)\n",
    "X_test = X_test.apply(clean_html)\n",
    "\n",
    "# Show a sample to check if HTML has been cleaned\n",
    "print(\"Sample cleaned message from training set:\")\n",
    "print(X_train.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize unicode characters (e.g., â, ä → a)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Remove inline JavaScript/CSS if any\n",
    "    text = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', text, flags=re.S)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "    \n",
    "    # Remove all HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove byte prefix \"b'\" if present\n",
    "    text = re.sub(r\"^b\\'\", '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned message after full preprocessing:\n",
      "thursday february pmsbwhoeopre urgent good news on ni sidbthat is wonderful kudos to gordon and shaun\n"
     ]
    }
   ],
   "source": [
    "# Apply the new clean_text function to training and test sets\n",
    "X_train = X_train.apply(clean_text)\n",
    "X_test = X_test.apply(clean_text)\n",
    "\n",
    "# Show a cleaned sample to verify\n",
    "print(\"Sample cleaned message after full preprocessing:\")\n",
    "print(X_train.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from a text string\n",
    "def remove_stopwords(text):\n",
    "    # Load English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Keep only words that are NOT stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned message sample:\n",
      "thursday february pmsbwhoeopre urgent good news ni sidbthat wonderful kudos gordon shaun\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(remove_stopwords)\n",
    "X_test = X_test.apply(remove_stopwords)\n",
    "\n",
    "print(\"Final cleaned message sample:\")\n",
    "print(X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data if not already present\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization to a text string\n",
    "def lemmatize_text(text):\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join back into a single string\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final preprocessed and lemmatized message:\n",
      "thursday february pmsbwhoeopre urgent good news ni sidbthat wonderful kudos gordon shaun\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatization to cleaned train and test sets\n",
    "X_train = X_train.apply(lemmatize_text)\n",
    "X_test = X_test.apply(lemmatize_text)\n",
    "\n",
    "# Show result\n",
    "print(\"Final preprocessed and lemmatized message:\")\n",
    "print(X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money: 730\n",
      "account: 608\n",
      "bank: 580\n",
      "fund: 561\n",
      "u: 520\n",
      "business: 406\n",
      "country: 365\n",
      "transaction: 323\n",
      "company: 318\n",
      "million: 306\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Join all texts into one string\n",
    "all_text = ' '.join(X_train.dropna())\n",
    "\n",
    "# Split into individual words\n",
    "words = all_text.split()\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Get the 10 most common words\n",
    "top_10_words = word_freq.most_common(10)\n",
    "\n",
    "# Display results\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X and y into a DataFrame to create data_train and data_val\n",
    "data_train = pd.DataFrame({'preprocessed_text': X_train, 'label': y_train})\n",
    "data_val = pd.DataFrame({'preprocessed_text': X_test, 'label': y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>thursday february pmsbwhoeopre urgent good new...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>fyimills cheryl monday april pmhfw decision up...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>dearcgood dayei know message come suprise cons...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>like</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>gooddaythanks response email today business in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     preprocessed_text  label  money_mark  \\\n",
       "687  thursday february pmsbwhoeopre urgent good new...      0           0   \n",
       "500  fyimills cheryl monday april pmhfw decision up...      0           0   \n",
       "332  dearcgood dayei know message come suprise cons...      1           1   \n",
       "979                                               like      0           0   \n",
       "817  gooddaythanks response email today business in...      1           0   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "687                 0        88  \n",
       "500                 0        50  \n",
       "332                 1       890  \n",
       "979                 0         4  \n",
       "817                 1       468  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add extra features to training and validation sets\n",
    "\n",
    "# Define lists of suspicious tokens\n",
    "money_symbol_list = \"euro|dollar|pound|\\\\€|\\\\$\"\n",
    "suspicious_words = \"|\".join([\n",
    "    \"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \n",
    "    \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"\n",
    "])\n",
    "\n",
    "# Add features to training set\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_symbol_list, regex=True) * 1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words, regex=True) * 1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "# Add features to validation set\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_symbol_list, regex=True) * 1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words, regex=True) * 1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "# Display updated training data\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell adds three new features to the train and validation dataframes:\n",
    "# 1. 'money_mark': a binary indicator showing whether the message contains symbols related to money  (such as \"euro\", \"dollar\", \"€\", \"$\", etc.).\n",
    "# 2. 'suspicious_words': a binary indicator showing whether the message contains common suspicious words  often associated with spam (like \"free\", \"money\", \"bank\", \"transaction\", etc.).\n",
    "# 3. 'text_len': the total number of characters in the preprocessed message, used as a numerical feature to help understand message length patterns in spam vs. ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you create a Bag of Words with the CountVectorizer method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'abacha' 'abandoned' 'abidjan' 'able' 'abroad' 'ac' 'accept'\n",
      " 'acceptance' 'access' 'according' 'account' 'accountant' 'accounti'\n",
      " 'accounting' 'acknowledge' 'act' 'action' 'actual']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the CountVectorizer with some preprocessing options\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000,         # Limit to top 1000 most frequent tokens\n",
    "    stop_words='english',      # Remove English stopwords\n",
    "    ngram_range=(1, 1)         # Use unigrams only\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_bow = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Transform the validation/test data using the same vectorizer\n",
    "X_val_bow = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Print the first 20 feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a Bag of Words using CountVectorizer\n",
    "# - Limits to top 1000 most frequent words\n",
    "# - Removes English stopwords\n",
    "# - Uses unigrams only (single words)\n",
    "# - Fits on training data and transforms both train and validation sets\n",
    "# - Prints the first 20 words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (800, 1000)\n",
      "TF-IDF shape (validation): (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Import the TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the TfidfVectorizer object\n",
    "# max_features=1000 limits the number of features to the 1000 most frequent ones\n",
    "# stop_words='english' removes common English words like 'the', 'is', etc.\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the training text and transform it into a TF-IDF matrix\n",
    "X_train_tfidf = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Do the same transformation on the validation set (no fitting again!)\n",
    "X_val_tfidf = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Print the shape of the transformed training dataset (rows = messages, columns = features)\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "\n",
    "# Print the shape of the transformed validation dataset\n",
    "print(\"TF-IDF shape (validation):\", X_val_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used TfidfVectorizer to convert the text into numerical features. It gives more weight to rare words and less to common ones.\n",
    "\n",
    "# First, we fitted the vectorizer on the training data.Then, we used the same vectorizer to transform the validation data (no fitting again!).\n",
    "\n",
    "# The output of vectorizer.transform() is a sparse matrix. Each row represents a message, and each column is a word (feature).\n",
    "\n",
    "# The shape (train): (number_of_messages, number_of_features)\n",
    "# For example: (4000, 1000) means 4000 messages and 1000 unique words used as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task (optional) - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "Use a MultinimialNB with default parameters.\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
